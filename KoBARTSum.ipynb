{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f205f077-501c-443b-9621-614f9365c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "import platform\n",
    "import itertools\n",
    "import collections\n",
    "import pkg_resources  # pip install py-rouge\n",
    "from io import open\n",
    "from rouge import Rouge\n",
    "from konlpy.tag import Mecab \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import transformers\n",
    "from transformers import get_scheduler, PreTrainedTokenizerFast, EarlyStoppingCallback, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfed46-d560-4ed1-a404-56476b58d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer arguments\n",
    "lr = 1e-3\n",
    "stop = 3\n",
    "epoch = 100\n",
    "batch = 4\n",
    "seed = 42\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749fa23-4c11-4378-ae4b-7a1f2cf36614",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52af1a5e-1e96-48b7-8ad4-9d4a797debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwsS3Downloader(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aws_access_key_id=None,\n",
    "        aws_secret_access_key=None,\n",
    "    ):\n",
    "        self.resource = boto3.Session(\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        ).resource(\"s3\")\n",
    "        self.client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "        )\n",
    "\n",
    "    def __split_url(self, url: str):\n",
    "        if url.startswith(\"s3://\"):\n",
    "            url = url.replace(\"s3://\", \"\")\n",
    "        bucket, key = url.split(\"/\", maxsplit=1)\n",
    "        return bucket, key\n",
    "\n",
    "    def download(self, url: str, local_dir: str):\n",
    "        bucket, key = self.__split_url(url)\n",
    "        filename = os.path.basename(key)\n",
    "        file_path = os.path.join(local_dir, filename)\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        meta_data = self.client.head_object(Bucket=bucket, Key=key)\n",
    "        total_length = int(meta_data.get(\"ContentLength\", 0))\n",
    "\n",
    "        downloaded = 0\n",
    "\n",
    "        def progress(chunk):\n",
    "            nonlocal downloaded\n",
    "            downloaded += chunk\n",
    "            done = int(50 * downloaded / total_length)\n",
    "            sys.stdout.write(\n",
    "                \"\\r{}[{}{}]\".format(file_path, \"█\" * done, \".\" * (50 - done))\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                self.client.download_fileobj(bucket, key, f, Callback=progress)\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.flush()\n",
    "        except:\n",
    "            raise Exception(f\"downloading file is failed. {url}\")\n",
    "        return file_path\n",
    "\n",
    "def download(url, chksum=None, cachedir=\".cache\"):\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    os.makedirs(cachedir_full, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "    file_path = os.path.join(cachedir_full, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        if hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10] == chksum:\n",
    "            print(f\"using cached model. {file_path}\")\n",
    "            return file_path, True\n",
    "\n",
    "    s3 = AwsS3Downloader()\n",
    "    file_path = s3.download(url, cachedir_full)\n",
    "    if chksum:\n",
    "        assert (\n",
    "            chksum == hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10]\n",
    "        ), \"corrupted file!\"\n",
    "    return file_path, False\n",
    "\n",
    "def get_kobart_tokenizer(cachedir=\".cache\"):\n",
    "    \"\"\"Get KoGPT2 Tokenizer file path after downloading\"\"\"\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/tokenizers/kobart_base_tokenizer_cased_cf74400bce.zip\",\n",
    "        \"chksum\": \"cf74400bce\",\n",
    "    }\n",
    "    file_path, is_cached = download(\n",
    "        tokenizer[\"url\"], tokenizer[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    if (\n",
    "        not os.path.exists(os.path.join(cachedir_full, \"emji_tokenizer\"))\n",
    "        or not is_cached\n",
    "    ):\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(\n",
    "                os.path.join(cachedir_full, \"emji_tokenizer\"), ignore_errors=True\n",
    "            )\n",
    "        zipf = ZipFile(os.path.expanduser(file_path))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    tok_path = os.path.join(cachedir_full, \"emji_tokenizer/model.json\")\n",
    "    tokenizer_obj = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tok_path,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "    )\n",
    "    return tokenizer_obj\n",
    "\n",
    "def get_pytorch_kobart_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    pytorch_kobart = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/models/kobart_base_cased_ff4bda5738.zip\",\n",
    "        \"chksum\": \"ff4bda5738\",\n",
    "    }\n",
    "    model_zip, is_cached = download(\n",
    "        pytorch_kobart[\"url\"], pytorch_kobart[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    model_path = os.path.join(cachedir_full, \"kobart_from_pretrained\")\n",
    "    if not os.path.exists(model_path) or not is_cached:\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(model_path, ignore_errors=True)\n",
    "        zipf = ZipFile(os.path.expanduser(model_zip))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d000bf5f-3255-4995-a8b5-0312a863809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(phase):\n",
    "    work_dir = \"C:\\\\Users\\\\hist\\\\Documents\\\\GitHub\\\\KoBART\"\n",
    "    if phase == 'train':\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/1.Training/라벨링데이터/TL1'\n",
    "    else:\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/2.Validation/라벨링데이터/VL1'\n",
    "    listdir = os.listdir(tmp)\n",
    "    df = pd.DataFrame({}, columns = ['genre', 'text', 'label'])\n",
    "    for i in listdir:\n",
    "        files = os.listdir(f'{tmp}/{i}/2~3sent')\n",
    "        for f in tqdm(files):\n",
    "            with open(f'{tmp}/{i}/2~3sent/{f}', 'r', encoding='utf-8') as json_file:\n",
    "                j = json.loads(json_file.read())\n",
    "                df2 = pd.DataFrame.from_dict([{'genre' : i, \n",
    "                                               'text'  : j['Meta(Refine)']['passage'], \n",
    "                                               'label' : j['Annotation']['summary1']}])\n",
    "                df = pd.concat([df, df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8e76f6-4842-4a72-9aa3-199d485162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train = make_df('train').reset_index(drop=True)\n",
    "# val = make_df('val').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6ee0bd-7fbe-48c4-baa2-9b3437f20c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_parquet('train.parquet')\n",
    "# val.to_parquet('val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c585aecd-26da-4842-a0bb-4c664674508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09.literature</td>\n",
       "      <td>우징이, 염장, 정년(閻長,鄭年) 등 여섯 장군으로 하여금 군사를 인솔하고 북를 두...</td>\n",
       "      <td>점령 지대를 넓히며 동진한 이 군대는 신라 대감 김민주 등 여러 곳에서 그곳 수장에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03.his_cul</td>\n",
       "      <td>태조산에 있는 절로, 고려 태조(재위 918~943) 때 도선국사가 처음 세웠다는 ...</td>\n",
       "      <td>설화에 따르면 백학 한 쌍이 대웅전 뒤쪽 암벽에 불상을 조각하고 날아간 자리에 도선...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           genre                                               text  \\\n",
       "0  09.literature  우징이, 염장, 정년(閻長,鄭年) 등 여섯 장군으로 하여금 군사를 인솔하고 북를 두...   \n",
       "1     03.his_cul  태조산에 있는 절로, 고려 태조(재위 918~943) 때 도선국사가 처음 세웠다는 ...   \n",
       "\n",
       "                                               label  \n",
       "0  점령 지대를 넓히며 동진한 이 군대는 신라 대감 김민주 등 여러 곳에서 그곳 수장에...  \n",
       "1  설화에 따르면 백학 한 쌍이 대웅전 뒤쪽 암벽에 불상을 조각하고 날아간 자리에 도선...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "val   = pd.read_parquet('val.parquet')\n",
    "val   = val.sample(n=2, replace=False).reset_index(drop=True)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236632ac-68ab-40f1-a345-df7feb5aebd2",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0df920-1331-4896-b591-0f8d1e536fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocss(df):\n",
    "    df.text = df.text.apply(lambda x : re.sub('\\n', ' ',  x))\n",
    "    df.text = df.text.apply(lambda x : re.sub(' +', ' ',  x).strip())\n",
    "    return df\n",
    "\n",
    "train = preprocss(train)\n",
    "val = preprocss(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b593e39-9bad-4a94-9253-a0ecfa3bde08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우징이, 염장, 정년(閻長,鄭年) 등 여섯 장군으로 하여금 군사를 인솔하고 북를 두드리며 무주성에 들 때 같은 때는 군용이 진실로 당당하고 성하였 다. 신왕인 김명에게는 반역군이요 국가적으로는 토역(討逆)군인 이 군대는, 착일착 점령지대를 넓히며 동진하였다. 여러 곳에서 그곳 수장(守將)에게 반항를 받았지만, 반항하는 자는 모두 참패하였다. 신라대감(新羅大監) 김민주(金敏周) 같은 사람은 적지 않은 군사를 끌어가 지고 반항를 하였지만 우징의 막하 장군 낙금, 이순행(駱金,李順行) 등이 마병(馬兵)으로써 돌격를 하여 이를 평정하였다. 토역군인지 반역군인지 장 차 결과를 보아야 밝혀질 우징의 군대는, 평동장군 김양의 지휘 아래, 옛날 의 국경도 무사히 넘어, 동진(東進)를 계속하여 새해 정월 열아흐렛날은 대 구(大丘)에 이르렀다. 인제는 서울도 지호 간이었다. 요 며칠 전에 선왕를 목매게 하고 스스로 서서 임금이 된 김명은, 관군(官 軍)를 호령하여 나아가 맞아 싸우게 하였다. 관군를 내어 보내기는 하고도 왕은 스스로 절망의 탄성를 연하여 내었다. 위에 오른 지 불과 수삼 삭, 아 직 내 백성 내 군사라고 믿를 만한 사람은 얻지 못하였는데, 돌연한 이 변 란이요, 더우기 변란의 주인인 우징은 그의 아버지의대부터 관민간에 많은 존경과 애모를 받던 사람이었다. 이러한 입장이매 분명한 패배요 멸망이다. 근신 시종들도 모두 도망가고, 겨우 붙들어 둔 두세 명를 데리고, 왕은 망 루(望樓)에서 형편 형세를 살피고 있었다. 대구를 벌써 지난 적군(?)이매, 관군과는 서울 근교에서 부딪칠 것이다. 부딪치면 그 결과는? 분명한 패배일 줄 알면서도, 그래도 천에 하나의 요행를 기다리는 왕은 시 종들과 교외 쪽를 바라보고 있었다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5314e71-d4c6-4854-9a55-f21ba19e5500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'점령 지대를 넓히며 동진한 이 군대는 신라 대감 김민주 등 여러 곳에서 그곳 수장에게 반항를 받았지만 반항하는 자는 모두 참패하였다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d04c8a62-20e5-4e03-a233-d46bd31414b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c8bce-85f3-4df7-86fd-a997b4cf45f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b74843-6691-4e39-acf2-628b7cc0bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "class KoBARTSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "\n",
    "        self.pad_index = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.df.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(instance['text'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tokenizer.encode(instance['label'])\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "        \n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "train_dataset = KoBARTSumDataset(train, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 1024)\n",
    "val_dataset = KoBARTSumDataset(val, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8de69c-1cd7-47e1-959a-52fd76a5212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1499, 857)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().max(), val.text.str.len().max(), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5d583-b424-4621-94dd-18b55d51bebb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecda25d-f454-4cca-b97e-7acd831c16b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('gogamza/kobart-base-v1').to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c5dc9d-e93d-435e-9428-3b1246a69071",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3311b471-69ec-41cf-b50f-a1006ce6dfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    print('Text\\n', val.text[0])\n",
    "    print(f'Summarize\\nGold : {labels[0]}\\nGen : {preds[0]}')\n",
    "    \n",
    "    labels = ['\\n'.join(labels)]\n",
    "    preds = ['\\n'.join(preds)]\n",
    "    score = rouge.get_scores(preds, labels, avg=True)\n",
    "    return {\n",
    "        \"ROUGE-1\" : score['rouge-1'],\n",
    "        \"ROUGE-2\" : score['rouge-2'],\n",
    "        \"ROUGE-L\" : score['rouge-l'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ffaa3fc-c480-4ec4-b3e7-95846b76f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=1, eta_min=0, last_epoch=-1)\n",
    "\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "                                                            num_warmup_steps=100, \n",
    "                                                            num_training_steps=epoch * len(train_dataset) * batch, \n",
    "                                                            last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06715e6-246b-45af-b86c-463d0b260cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 73340\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 114500\n",
      "  Number of trainable parameters = 123859968\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(run_name = f'KoBARTSum',\n",
    "                                output_dir= f\"models\",\n",
    "                                evaluation_strategy=\"steps\",\n",
    "                                eval_steps=10,\n",
    "                                save_steps=10,\n",
    "                                logging_steps=10,\n",
    "                                save_total_limit = 2,\n",
    "                                \n",
    "                                per_device_train_batch_size=batch,\n",
    "                                per_device_eval_batch_size=batch,\n",
    "                                gradient_accumulation_steps=16,\n",
    "                                num_train_epochs=epoch,\n",
    "                                \n",
    "                                load_best_model_at_end=True,\n",
    "                                fp16=True,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                predict_with_generate=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         args=args,\n",
    "\n",
    "                         train_dataset=train_dataset,\n",
    "                         eval_dataset=val_dataset,\n",
    "\n",
    "                         compute_metrics=compute_metrics,\n",
    "                         optimizers=(optimizer, lr_scheduler),\n",
    "                         callbacks=[EarlyStoppingCallback(early_stopping_patience=stop)],\n",
    "                         data_collator=collator,)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bcfb0-64aa-4442-9a69-bfa7dee75541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91609e7-cddf-4b59-93df-8694f525c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
