{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f205f077-501c-443b-9621-614f9365c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "import platform\n",
    "import itertools\n",
    "import collections\n",
    "import pkg_resources  # pip install py-rouge\n",
    "from io import open\n",
    "from konlpy.tag import Mecab \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, EarlyStoppingCallback, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfed46-d560-4ed1-a404-56476b58d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer arguments\n",
    "lr = 1e-4\n",
    "stop = 3\n",
    "epoch = 1000\n",
    "batch = 2\n",
    "seed = 42\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749fa23-4c11-4378-ae4b-7a1f2cf36614",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52af1a5e-1e96-48b7-8ad4-9d4a797debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwsS3Downloader(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aws_access_key_id=None,\n",
    "        aws_secret_access_key=None,\n",
    "    ):\n",
    "        self.resource = boto3.Session(\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        ).resource(\"s3\")\n",
    "        self.client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "        )\n",
    "\n",
    "    def __split_url(self, url: str):\n",
    "        if url.startswith(\"s3://\"):\n",
    "            url = url.replace(\"s3://\", \"\")\n",
    "        bucket, key = url.split(\"/\", maxsplit=1)\n",
    "        return bucket, key\n",
    "\n",
    "    def download(self, url: str, local_dir: str):\n",
    "        bucket, key = self.__split_url(url)\n",
    "        filename = os.path.basename(key)\n",
    "        file_path = os.path.join(local_dir, filename)\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        meta_data = self.client.head_object(Bucket=bucket, Key=key)\n",
    "        total_length = int(meta_data.get(\"ContentLength\", 0))\n",
    "\n",
    "        downloaded = 0\n",
    "\n",
    "        def progress(chunk):\n",
    "            nonlocal downloaded\n",
    "            downloaded += chunk\n",
    "            done = int(50 * downloaded / total_length)\n",
    "            sys.stdout.write(\n",
    "                \"\\r{}[{}{}]\".format(file_path, \"█\" * done, \".\" * (50 - done))\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                self.client.download_fileobj(bucket, key, f, Callback=progress)\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.flush()\n",
    "        except:\n",
    "            raise Exception(f\"downloading file is failed. {url}\")\n",
    "        return file_path\n",
    "\n",
    "def download(url, chksum=None, cachedir=\".cache\"):\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    os.makedirs(cachedir_full, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "    file_path = os.path.join(cachedir_full, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        if hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10] == chksum:\n",
    "            print(f\"using cached model. {file_path}\")\n",
    "            return file_path, True\n",
    "\n",
    "    s3 = AwsS3Downloader()\n",
    "    file_path = s3.download(url, cachedir_full)\n",
    "    if chksum:\n",
    "        assert (\n",
    "            chksum == hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10]\n",
    "        ), \"corrupted file!\"\n",
    "    return file_path, False\n",
    "\n",
    "def get_kobart_tokenizer(cachedir=\".cache\"):\n",
    "    \"\"\"Get KoGPT2 Tokenizer file path after downloading\"\"\"\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/tokenizers/kobart_base_tokenizer_cased_cf74400bce.zip\",\n",
    "        \"chksum\": \"cf74400bce\",\n",
    "    }\n",
    "    file_path, is_cached = download(\n",
    "        tokenizer[\"url\"], tokenizer[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    if (\n",
    "        not os.path.exists(os.path.join(cachedir_full, \"emji_tokenizer\"))\n",
    "        or not is_cached\n",
    "    ):\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(\n",
    "                os.path.join(cachedir_full, \"emji_tokenizer\"), ignore_errors=True\n",
    "            )\n",
    "        zipf = ZipFile(os.path.expanduser(file_path))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    tok_path = os.path.join(cachedir_full, \"emji_tokenizer/model.json\")\n",
    "    tokenizer_obj = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tok_path,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "    )\n",
    "    return tokenizer_obj\n",
    "\n",
    "def get_pytorch_kobart_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    pytorch_kobart = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/models/kobart_base_cased_ff4bda5738.zip\",\n",
    "        \"chksum\": \"ff4bda5738\",\n",
    "    }\n",
    "    model_zip, is_cached = download(\n",
    "        pytorch_kobart[\"url\"], pytorch_kobart[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    model_path = os.path.join(cachedir_full, \"kobart_from_pretrained\")\n",
    "    if not os.path.exists(model_path) or not is_cached:\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(model_path, ignore_errors=True)\n",
    "        zipf = ZipFile(os.path.expanduser(model_zip))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d000bf5f-3255-4995-a8b5-0312a863809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(phase):\n",
    "    work_dir = \"C:\\\\Users\\\\hist\\\\Documents\\\\GitHub\\\\KoBART\"\n",
    "    if phase == 'train':\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/1.Training/라벨링데이터/TL1'\n",
    "    else:\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/2.Validation/라벨링데이터/VL1'\n",
    "    listdir = os.listdir(tmp)\n",
    "    df = pd.DataFrame({}, columns = ['genre', 'text', 'label'])\n",
    "    for i in listdir:\n",
    "        files = os.listdir(f'{tmp}/{i}/2~3sent')\n",
    "        for f in tqdm(files):\n",
    "            with open(f'{tmp}/{i}/2~3sent/{f}', 'r', encoding='utf-8') as json_file:\n",
    "                j = json.loads(json_file.read())\n",
    "                df2 = pd.DataFrame.from_dict([{'genre' : i, \n",
    "                                               'text'  : j['Meta(Refine)']['passage'], \n",
    "                                               'label' : j['Annotation']['summary1']}])\n",
    "                df = pd.concat([df, df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8e76f6-4842-4a72-9aa3-199d485162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train = make_df('train').reset_index(drop=True)\n",
    "# val = make_df('val').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6ee0bd-7fbe-48c4-baa2-9b3437f20c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_parquet('train.parquet')\n",
    "# val.to_parquet('val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c585aecd-26da-4842-a0bb-4c664674508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03.his_cul</td>\n",
       "      <td>충청남도 홍성군 홍성읍 대교리에 있는 불상으로 광경사지 미륵불이라고도 불린다.  머...</td>\n",
       "      <td>광경사지 미륵불은 신체 윤곽을 선으로 표현하고 앞면만 조각한 불상으로 비례 감각이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09.literature</td>\n",
       "      <td>소녀 1 어서 겨울이 왔으면 하는 것이 소녀의 기원이었다. 하루에 밤이 두 번이고 ...</td>\n",
       "      <td>소녀는 하루에 밤이 여러 번 왔으면 할 정도로 눈이 쏟아지고 물이 얼어붙는 겨울이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08.speech</td>\n",
       "      <td>성취평가제 여기에 원점수, 과목평균, 표준편차 등 전체 병기를 해서 다양한 정보를 ...</td>\n",
       "      <td>국가공통교육과정을 적용하고 있음에도 기본 잣대가 정부 차원에서 정확하게 제공되지 못...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.news_r</td>\n",
       "      <td>한국의 지난달 소비자 물가가 사상 두 번째로 ‘마이너스’를 기록했다.\\n  유로화를...</td>\n",
       "      <td>코로나19로 전 세계를 덮친 저유가의 영향과 사회적 거리두기로 인한 수요의 감소로 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03.his_cul</td>\n",
       "      <td>태백 용연굴은 석회암으로 이루어진 동굴로 총 길이는 800m이며, 지금으로부터 약 ...</td>\n",
       "      <td>임진왜란 때 의병들의 본부역할을 하던 태백 용연굴은 6종의 새로운 동굴생물이 발견된...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>09.literature</td>\n",
       "      <td>‘죽음은 갈색이다. 그리구…….” 더 모르게 된다. “아이 죽겠구나, 죽겠구나.”...</td>\n",
       "      <td>안락의자에서 일어나 흰 석회 벽과 갈색 기둥이 끝없는 대기로 변할 때 나는 쾅 하고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>08.speech</td>\n",
       "      <td>안전한 접종을 위해 기저질환이 있거나 12주 미만의 임신 초기인 경우에는 접종 전에...</td>\n",
       "      <td>안전한 접종을 위해 12주 미만의 임신 초기인 경우에는 접종 전 임산부와 태아의 상...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>05.minute</td>\n",
       "      <td>정운천 위원] \"아니 기대했던 장관님 얘기를 대면…… 이개호 장관은 이 자리에서 분...</td>\n",
       "      <td>김 위원이 한농대는 농생명융합도시의 상징이며 해체한다면 각 지역의 특장점을 살리려는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>05.minute</td>\n",
       "      <td>문화체육관광부장관 박양우] \"예.\"\\n위원장 안민석] \"수고하셨습니다.  다음으로 ...</td>\n",
       "      <td>이 위원은 무작정 도쿄올림픽을 보이콧하는 것은 이롭지 않다고 생각한다고 말했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>08.speech</td>\n",
       "      <td>한편, 191명의 담임교사를 대상으로 한 조사에서는 교사 2명 중 1명이 다문화가족...</td>\n",
       "      <td>대다수의 담임교사들이 다문화가족 학생을 위한 프로그램의 필요성을 인식하고 농촌진흥청...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            genre                                               text  \\\n",
       "0      03.his_cul  충청남도 홍성군 홍성읍 대교리에 있는 불상으로 광경사지 미륵불이라고도 불린다.  머...   \n",
       "1   09.literature  소녀 1 어서 겨울이 왔으면 하는 것이 소녀의 기원이었다. 하루에 밤이 두 번이고 ...   \n",
       "2       08.speech  성취평가제 여기에 원점수, 과목평균, 표준편차 등 전체 병기를 해서 다양한 정보를 ...   \n",
       "3       01.news_r  한국의 지난달 소비자 물가가 사상 두 번째로 ‘마이너스’를 기록했다.\\n  유로화를...   \n",
       "4      03.his_cul  태백 용연굴은 석회암으로 이루어진 동굴로 총 길이는 800m이며, 지금으로부터 약 ...   \n",
       "..            ...                                                ...   \n",
       "95  09.literature   ‘죽음은 갈색이다. 그리구…….” 더 모르게 된다. “아이 죽겠구나, 죽겠구나.”...   \n",
       "96      08.speech  안전한 접종을 위해 기저질환이 있거나 12주 미만의 임신 초기인 경우에는 접종 전에...   \n",
       "97      05.minute  정운천 위원] \"아니 기대했던 장관님 얘기를 대면…… 이개호 장관은 이 자리에서 분...   \n",
       "98      05.minute  문화체육관광부장관 박양우] \"예.\"\\n위원장 안민석] \"수고하셨습니다.  다음으로 ...   \n",
       "99      08.speech  한편, 191명의 담임교사를 대상으로 한 조사에서는 교사 2명 중 1명이 다문화가족...   \n",
       "\n",
       "                                                label  \n",
       "0   광경사지 미륵불은 신체 윤곽을 선으로 표현하고 앞면만 조각한 불상으로 비례 감각이 ...  \n",
       "1   소녀는 하루에 밤이 여러 번 왔으면 할 정도로 눈이 쏟아지고 물이 얼어붙는 겨울이 ...  \n",
       "2   국가공통교육과정을 적용하고 있음에도 기본 잣대가 정부 차원에서 정확하게 제공되지 못...  \n",
       "3   코로나19로 전 세계를 덮친 저유가의 영향과 사회적 거리두기로 인한 수요의 감소로 ...  \n",
       "4   임진왜란 때 의병들의 본부역할을 하던 태백 용연굴은 6종의 새로운 동굴생물이 발견된...  \n",
       "..                                                ...  \n",
       "95  안락의자에서 일어나 흰 석회 벽과 갈색 기둥이 끝없는 대기로 변할 때 나는 쾅 하고...  \n",
       "96  안전한 접종을 위해 12주 미만의 임신 초기인 경우에는 접종 전 임산부와 태아의 상...  \n",
       "97  김 위원이 한농대는 농생명융합도시의 상징이며 해체한다면 각 지역의 특장점을 살리려는...  \n",
       "98       이 위원은 무작정 도쿄올림픽을 보이콧하는 것은 이롭지 않다고 생각한다고 말했다.  \n",
       "99  대다수의 담임교사들이 다문화가족 학생을 위한 프로그램의 필요성을 인식하고 농촌진흥청...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "val = pd.read_parquet('val.parquet')\n",
    "val = val.sample(n=100, replace=False).reset_index(drop=True)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236632ac-68ab-40f1-a345-df7feb5aebd2",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0df920-1331-4896-b591-0f8d1e536fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocss(df):\n",
    "    df.text = df.text.apply(lambda x : re.sub('\\n', ' ',  x))\n",
    "    df.text = df.text.apply(lambda x : re.sub(' +', ' ',  x).strip())\n",
    "    return df\n",
    "\n",
    "train = preprocss(train)\n",
    "val = preprocss(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b593e39-9bad-4a94-9253-a0ecfa3bde08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'충청남도 홍성군 홍성읍 대교리에 있는 불상으로 광경사지 미륵불이라고도 불린다. 머리에는 작은 소라 모양의 머리칼을 붙여 놓았고, 얼굴은 눈·코·입을 낮게 돋을새김하였다. 주먹코와 두툼한 입술, 길다란 눈 등이 매우 익살스러운 표정을 만들고 있다. 신체의 윤곽은 선으로 표현했으며, 앞면에만 조각을 하였다. 양 어깨를 감싼 옷을 걸치고 있으며, 가슴에서 발까지 U자형의 옷주름을 새겼다. 오른손은 가슴에 붙이고 왼손은 손바닥이 보이게 들고 있다. 조각수법이 거칠고 비례감각이 없는 점으로 보아 조선시대에 민간신앙의 대상으로 만들어진 듯하다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5314e71-d4c6-4854-9a55-f21ba19e5500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광경사지 미륵불은 신체 윤곽을 선으로 표현하고 앞면만 조각한 불상으로 비례 감각이 없고 조각 수법이 거칠어 민간신앙의 대상으로 조선시대에 만들어진 듯하다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c8bce-85f3-4df7-86fd-a997b4cf45f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ced9f18-c3aa-4807-8047-351cb8a90d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBARTSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "\n",
    "        self.pad_index = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.df.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(instance['text'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tokenizer.encode(instance['label'])\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "    \n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'labels': np.array(dec_input_ids, dtype=np.int_),}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b74843-6691-4e39-acf2-628b7cc0bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\hist\\Documents\\GitHub\\KoBART\\.cache\\kobart_base_tokenizer_cased_cf74400bce.zip\n",
      "using cached model. C:\\Users\\hist\\Documents\\GitHub\\KoBART\\.cache\\kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KoBARTSumDataset(train, get_kobart_tokenizer(), 512)\n",
    "val_dataset = KoBARTSumDataset(val, get_kobart_tokenizer(), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4173ad-7571-4186-aa01-18d32724cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([16735, 12335, 11821, 14031, 10952, 11841, 11810, 14299, 14143,\n",
       "        16121, 15991, 19858,  1543, 18044, 11914, 14085, 10770,  9092,\n",
       "        18025, 10496, 15116, 19211, 14141, 16749, 14862, 14245, 14243,\n",
       "        16344, 14671, 14725, 14483, 11471,   245, 16735, 10952, 11810,\n",
       "        17301, 12007, 14067,  9034, 14143, 16121, 15991, 19858, 18463,\n",
       "        22075, 23658, 14189, 16367, 19775, 11214, 14807, 15425, 25437,\n",
       "        12005, 21308, 11786, 14114, 12332, 21245, 14174, 11264, 11950,\n",
       "        14101, 15460, 12074, 14105, 12005, 18817, 11846, 10226, 14130,\n",
       "        16414, 28403, 19790, 17454, 13469, 17242, 14311, 23449, 25891,\n",
       "         9714,   243, 14040, 16267, 14075, 10500, 10788, 12060, 13590,\n",
       "        16338, 11786, 15859, 12007, 16904, 16728, 16261, 22075, 19727,\n",
       "        16922, 17125, 14130, 26294, 14038, 27368, 14328, 14048, 14038,\n",
       "        13672, 12333, 14410, 20122, 14736, 18154, 14144, 16626, 14737,\n",
       "        16527, 15735,  9103, 14159, 14532, 11950, 14130, 14281, 16781,\n",
       "        15962, 15262,  9760, 14160, 12346, 16247, 20388, 15196, 14108,\n",
       "        28403, 14196,  9993,  9066, 14673,  9698, 19454, 22887, 15029,\n",
       "        27375, 14346, 20602, 14583, 21368, 12005, 23962, 14959, 15135,\n",
       "        14473, 12034,   373, 11028, 10770,  9092,   239, 12865, 12034,\n",
       "        16589,   373, 11028, 19132,   240, 15196, 14108, 10215, 14244,\n",
       "        16247, 14169,  9123, 11908, 14038, 16277, 15063, 14025, 10671,\n",
       "        11810, 16863, 11863, 14270, 16735, 12471, 14248, 10512,  8981,\n",
       "        14083, 10293, 12034, 16634, 24294, 29815, 15033, 15615, 14038,\n",
       "         9133, 13758, 14288, 14527, 14596, 14318, 19754, 17942, 16954,\n",
       "        16338, 11264, 14222, 18433, 14863, 16636, 15442, 27134, 16247,\n",
       "        17507, 11841, 14957, 23412, 14325, 15302, 26294, 14038, 18173,\n",
       "        22809, 14561, 15964, 14196, 15521, 14143, 12865, 10770, 16058,\n",
       "        17045, 21766, 12037, 14775,  9160, 11699, 23373, 14188, 19290,\n",
       "        25686, 14040, 19887, 28816, 14449, 14059, 16247, 14554, 14038,\n",
       "        15699, 14025, 13331,  9501]),\n",
       " 'labels': array([    1, 14447, 15991, 11806, 15396, 14091, 22075, 15392, 11786,\n",
       "        27368, 14554, 14038, 14328, 14048, 14410, 20122, 21354, 17400,\n",
       "        14144, 16626, 17483, 16527, 15735,  9103, 14159, 14174, 11950,\n",
       "        14297, 14596, 14527, 14038, 14288, 14517, 19754,  1700,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5d583-b424-4621-94dd-18b55d51bebb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8de69c-1cd7-47e1-959a-52fd76a5212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1499, 1293)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().max(), val.text.str.len().max(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47006c36-ea6c-40f8-9614-80cfd680f0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Rouge:\n",
    "    DEFAULT_METRICS = {\"rouge-n\"}\n",
    "    DEFAULT_N = 1\n",
    "    STATS = [\"f\", \"p\", \"r\"]\n",
    "    AVAILABLE_METRICS = {\"rouge-n\", \"rouge-l\", \"rouge-w\"}\n",
    "    AVAILABLE_LENGTH_LIMIT_TYPES = {\"words\", \"bytes\"}\n",
    "    REMOVE_CHAR_PATTERN = re.compile(\"[^A-Za-z0-9가-힣]\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics=None,\n",
    "        max_n=None,\n",
    "        limit_length=True,\n",
    "        length_limit=1000,\n",
    "        length_limit_type=\"words\",\n",
    "        apply_avg=True,\n",
    "        apply_best=False,\n",
    "        use_tokenizer=True,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        self.metrics = metrics[:] if metrics is not None else Rouge.DEFAULT_METRICS\n",
    "        for m in self.metrics:\n",
    "            if m not in Rouge.AVAILABLE_METRICS:\n",
    "                raise ValueError(\"Unknown metric '{}'\".format(m))\n",
    "\n",
    "\n",
    "        self.max_n = max_n if \"rouge-n\" in self.metrics else None\n",
    "        # Add all rouge-n metrics\n",
    "        if self.max_n is not None:\n",
    "            index_rouge_n = self.metrics.index(\"rouge-n\")\n",
    "            del self.metrics[index_rouge_n]\n",
    "            self.metrics += [\"rouge-{}\".format(n) for n in range(1, self.max_n + 1)]\n",
    "        self.metrics = set(self.metrics)\n",
    "\n",
    "\n",
    "        self.limit_length = limit_length\n",
    "        if self.limit_length:\n",
    "            if length_limit_type not in Rouge.AVAILABLE_LENGTH_LIMIT_TYPES:\n",
    "                raise ValueError(\"Unknown length_limit_type '{}'\".format(length_limit_type))\n",
    "\n",
    "\n",
    "        self.length_limit = length_limit\n",
    "        if self.length_limit == 0:\n",
    "            self.limit_length = False\n",
    "        self.length_limit_type = length_limit_type\n",
    "\n",
    "\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        if use_tokenizer:\n",
    "            self.tokenizer = Mecab()\n",
    "\n",
    "\n",
    "        self.apply_avg = apply_avg\n",
    "        self.apply_best = apply_best\n",
    "        self.alpha = alpha\n",
    "        self.weight_factor = weight_factor\n",
    "        if self.weight_factor <= 0:\n",
    "            raise ValueError(\"ROUGE-W weight factor must greater than 0.\")\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        if self.use_tokenizer:\n",
    "            return self.tokenizer.morphs(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def split_into_sentences(text):\n",
    "        return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_ngrams(n, text):\n",
    "        ngram_set = collections.defaultdict(int)\n",
    "        max_index_ngram_start = len(text) - n\n",
    "        for i in range(max_index_ngram_start + 1):\n",
    "            ngram_set[tuple(text[i : i + n])] += 1\n",
    "        return ngram_set\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_into_words(sentences):\n",
    "        return list(itertools.chain(*[_.split() for _ in sentences]))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_ngrams_and_length(n, sentences):\n",
    "        assert len(sentences) > 0\n",
    "        assert n > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        return Rouge._get_ngrams(n, tokens), tokens, len(tokens) - (n - 1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unigrams(sentences):\n",
    "        assert len(sentences) > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        unigram_set = collections.defaultdict(int)\n",
    "        for token in tokens:\n",
    "            unigram_set[token] += 1\n",
    "        return unigram_set, len(tokens)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_p_r_f_score(\n",
    "        evaluated_count,\n",
    "        reference_count,\n",
    "        overlapping_count,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        precision = 0.0 if evaluated_count == 0 else overlapping_count / float(evaluated_count)\n",
    "        if weight_factor != 1.0:\n",
    "            precision = precision ** (1.0 / weight_factor)\n",
    "        recall = 0.0 if reference_count == 0 else overlapping_count / float(reference_count)\n",
    "        if weight_factor != 1.0:\n",
    "            recall = recall ** (1.0 / weight_factor)\n",
    "        f1_score = Rouge._compute_f_score(precision, recall, alpha)\n",
    "        return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_f_score(precision, recall, alpha=0.5):\n",
    "        return (\n",
    "            0.0\n",
    "            if (recall == 0.0 or precision == 0.0)\n",
    "            else precision * recall / ((1 - alpha) * precision + alpha * recall)\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams(evaluated_sentences, reference_sentences, n):\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_ngrams, _, evaluated_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, evaluated_sentences\n",
    "        )\n",
    "        reference_ngrams, _, reference_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, reference_sentences\n",
    "        )\n",
    "\n",
    "\n",
    "        # Gets the overlapping ngrams between evaluated and reference\n",
    "        overlapping_ngrams = set(evaluated_ngrams.keys()).intersection(set(reference_ngrams.keys()))\n",
    "        overlapping_count = 0\n",
    "        for ngram in overlapping_ngrams:\n",
    "            overlapping_count += min(evaluated_ngrams[ngram], reference_ngrams[ngram])\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams_lcs(evaluated_sentences, reference_sentences, weight_factor=1.0):\n",
    "        def _lcs(x, y):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(int)\n",
    "            dirs = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j - 1] + 1\n",
    "                        dirs[i, j] = \"|\"\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _wlcs(x, y, weight_factor):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(float)\n",
    "            dirs = collections.defaultdict(int)\n",
    "            lengths = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        length_tmp = lengths[i - 1, j - 1]\n",
    "                        vals[i, j] = (\n",
    "                            vals[i - 1, j - 1]\n",
    "                            + (length_tmp + 1) ** weight_factor\n",
    "                            - length_tmp ** weight_factor\n",
    "                        )\n",
    "                        dirs[i, j] = \"|\"\n",
    "                        lengths[i, j] = length_tmp + 1\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                        lengths[i, j] = 0\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "                        lengths[i, j] = 0\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _mark_lcs(mask, dirs, m, n):\n",
    "            while m != 0 and n != 0:\n",
    "                if dirs[m, n] == \"|\":\n",
    "                    m -= 1\n",
    "                    n -= 1\n",
    "                    mask[m] = 1\n",
    "                elif dirs[m, n] == \"^\":\n",
    "                    m -= 1\n",
    "                elif dirs[m, n] == \"<\":\n",
    "                    n -= 1\n",
    "                else:\n",
    "                    raise UnboundLocalError(\"Illegal move\")\n",
    "\n",
    "\n",
    "            return mask\n",
    "\n",
    "\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_unigrams_dict, evaluated_count = Rouge._get_unigrams(evaluated_sentences)\n",
    "        reference_unigrams_dict, reference_count = Rouge._get_unigrams(reference_sentences)\n",
    "\n",
    "\n",
    "        # Has to use weight factor for WLCS\n",
    "        use_WLCS = weight_factor != 1.0\n",
    "        if use_WLCS:\n",
    "            evaluated_count = evaluated_count ** weight_factor\n",
    "            reference_count = 0\n",
    "\n",
    "\n",
    "        overlapping_count = 0.0\n",
    "        for reference_sentence in reference_sentences:\n",
    "            reference_sentence_tokens = reference_sentence.split()\n",
    "            if use_WLCS:\n",
    "                reference_count += len(reference_sentence_tokens) ** weight_factor\n",
    "            hit_mask = [0 for _ in range(len(reference_sentence_tokens))]\n",
    "\n",
    "\n",
    "            for evaluated_sentence in evaluated_sentences:\n",
    "                evaluated_sentence_tokens = evaluated_sentence.split()\n",
    "\n",
    "\n",
    "                if use_WLCS:\n",
    "                    _, lcs_dirs = _wlcs(\n",
    "                        reference_sentence_tokens,\n",
    "                        evaluated_sentence_tokens,\n",
    "                        weight_factor,\n",
    "                    )\n",
    "                else:\n",
    "                    _, lcs_dirs = _lcs(reference_sentence_tokens, evaluated_sentence_tokens)\n",
    "                _mark_lcs(\n",
    "                    hit_mask,\n",
    "                    lcs_dirs,\n",
    "                    len(reference_sentence_tokens),\n",
    "                    len(evaluated_sentence_tokens),\n",
    "                )\n",
    "\n",
    "\n",
    "            overlapping_count_length = 0\n",
    "            for ref_token_id, val in enumerate(hit_mask):\n",
    "                if val == 1:\n",
    "                    token = reference_sentence_tokens[ref_token_id]\n",
    "                    if evaluated_unigrams_dict[token] > 0 and reference_unigrams_dict[token] > 0:\n",
    "                        evaluated_unigrams_dict[token] -= 1\n",
    "                        reference_unigrams_dict[ref_token_id] -= 1\n",
    "\n",
    "\n",
    "                        if use_WLCS:\n",
    "                            overlapping_count_length += 1\n",
    "                            if (\n",
    "                                ref_token_id + 1 < len(hit_mask) and hit_mask[ref_token_id + 1] == 0\n",
    "                            ) or ref_token_id + 1 == len(hit_mask):\n",
    "                                overlapping_count += overlapping_count_length ** weight_factor\n",
    "                                overlapping_count_length = 0\n",
    "                        else:\n",
    "                            overlapping_count += 1\n",
    "\n",
    "\n",
    "        if use_WLCS:\n",
    "            reference_count = reference_count ** weight_factor\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    def get_scores(self, hypothesis, references):\n",
    "        if isinstance(hypothesis, str):\n",
    "            hypothesis, references = [hypothesis], [references]\n",
    "\n",
    "\n",
    "        if type(hypothesis) != type(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' are not of the same type\")\n",
    "\n",
    "\n",
    "        if len(hypothesis) != len(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' do not have the same length\")\n",
    "        scores = {}\n",
    "        has_rouge_n_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]) > 0\n",
    "        )\n",
    "        if has_rouge_n_metric:\n",
    "            scores.update(self._get_scores_rouge_n(hypothesis, references))\n",
    "            # scores = {**scores, **self._get_scores_rouge_n(hypothesis, references)}\n",
    "\n",
    "\n",
    "        has_rouge_l_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"l\"]) > 0\n",
    "        )\n",
    "        if has_rouge_l_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, False))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, False)}\n",
    "\n",
    "\n",
    "        has_rouge_w_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"w\"]) > 0\n",
    "        )\n",
    "        if has_rouge_w_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, True))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, True)}\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_n(self, all_hypothesis, all_references):\n",
    "        metrics = [metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]\n",
    "\n",
    "\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS} for metric in metrics}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "                for metric in metrics\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis, references) in enumerate(zip(all_hypothesis, all_references)):\n",
    "            assert isinstance(hypothesis, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references, list):\n",
    "                has_multiple_references = len(references) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references = references[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis = self._preprocess_summary_as_a_whole(hypothesis)\n",
    "            references = (\n",
    "                [self._preprocess_summary_as_a_whole(reference) for reference in references]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_as_a_whole(references)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            for metric in metrics:\n",
    "                suffix = metric.split(\"-\")[-1]\n",
    "                n = int(suffix)\n",
    "\n",
    "\n",
    "                # Aggregate\n",
    "                if self.apply_avg:\n",
    "                    # average model\n",
    "                    total_hypothesis_ngrams_count = 0\n",
    "                    total_reference_ngrams_count = 0\n",
    "                    total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                    for reference in references:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                        total_hypothesis_ngrams_count += hypothesis_count\n",
    "                        total_reference_ngrams_count += reference_count\n",
    "                        total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                    score = Rouge._compute_p_r_f_score(\n",
    "                        total_hypothesis_ngrams_count,\n",
    "                        total_reference_ngrams_count,\n",
    "                        total_ngrams_overlapping_count,\n",
    "                        self.alpha,\n",
    "                    )\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += score[stat]\n",
    "                else:\n",
    "                    # Best model\n",
    "                    if self.apply_best:\n",
    "                        best_current_score = None\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][stat] += best_current_score[stat]\n",
    "                    # Keep all\n",
    "                    else:\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            for stat in Rouge.STATS:\n",
    "                                scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for metric in metrics:\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_l_or_w(self, all_hypothesis, all_references, use_w=False):\n",
    "        metric = \"rouge-w\" if use_w else \"rouge-l\"\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS}}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis_sentences, references_sentences) in enumerate(\n",
    "            zip(all_hypothesis, all_references)\n",
    "        ):\n",
    "            assert isinstance(hypothesis_sentences, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references_sentences, list):\n",
    "                has_multiple_references = len(references_sentences) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references_sentences = references_sentences[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis_sentences = self._preprocess_summary_per_sentence(hypothesis_sentences)\n",
    "            references_sentences = (\n",
    "                [\n",
    "                    self._preprocess_summary_per_sentence(reference)\n",
    "                    for reference in references_sentences\n",
    "                ]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_per_sentence(references_sentences)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            # Aggregate\n",
    "            if self.apply_avg:\n",
    "                # average model\n",
    "                total_hypothesis_ngrams_count = 0\n",
    "                total_reference_ngrams_count = 0\n",
    "                total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                for reference_sentences in references_sentences:\n",
    "                    (\n",
    "                        hypothesis_count,\n",
    "                        reference_count,\n",
    "                        overlapping_ngrams,\n",
    "                    ) = Rouge._compute_ngrams_lcs(\n",
    "                        hypothesis_sentences,\n",
    "                        reference_sentences,\n",
    "                        self.weight_factor if use_w else 1.0,\n",
    "                    )\n",
    "                    total_hypothesis_ngrams_count += hypothesis_count\n",
    "                    total_reference_ngrams_count += reference_count\n",
    "                    total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                score = Rouge._compute_p_r_f_score(\n",
    "                    total_hypothesis_ngrams_count,\n",
    "                    total_reference_ngrams_count,\n",
    "                    total_ngrams_overlapping_count,\n",
    "                    self.alpha,\n",
    "                    self.weight_factor if use_w else 1.0,\n",
    "                )\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] += score[stat]\n",
    "            else:\n",
    "                # Best model\n",
    "                if self.apply_best:\n",
    "                    best_current_score = None\n",
    "                    best_current_score_wlcs = None\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            total_hypothesis_ngrams_count,\n",
    "                            total_reference_ngrams_count,\n",
    "                            total_ngrams_overlapping_count,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        if use_w:\n",
    "                            reference_count_for_score = reference_count ** (\n",
    "                                1.0 / self.weight_factor\n",
    "                            )\n",
    "                            overlapping_ngrams_for_score = overlapping_ngrams\n",
    "                            score_wlcs = (\n",
    "                                overlapping_ngrams_for_score / reference_count_for_score\n",
    "                            ) ** (1.0 / self.weight_factor)\n",
    "\n",
    "\n",
    "                            if (\n",
    "                                best_current_score_wlcs is None\n",
    "                                or score_wlcs > best_current_score_wlcs\n",
    "                            ):\n",
    "                                best_current_score = score\n",
    "                                best_current_score_wlcs = score_wlcs\n",
    "                        else:\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += best_current_score[stat]\n",
    "                # Keep all\n",
    "                else:\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for stat in Rouge.STATS:\n",
    "                scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _preprocess_summary_as_a_whole(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                summary = \" \".join(sentences)\n",
    "                all_tokens = summary.split()  # Counting as in the perls script\n",
    "                summary = \" \".join(all_tokens[: self.length_limit])\n",
    "\n",
    "\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                summary = \"\"\n",
    "                current_len = 0\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "\n",
    "\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        if current_len != 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        if current_len > 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence[: self.length_limit - current_len]\n",
    "                        break\n",
    "        else:\n",
    "            summary = \" \".join(sentences)\n",
    "\n",
    "\n",
    "        summary = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary.lower()).strip()\n",
    "\n",
    "\n",
    "        tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary))\n",
    "        preprocessed_summary = [\" \".join(tokens)]\n",
    "\n",
    "\n",
    "        return preprocessed_summary\n",
    "\n",
    "\n",
    "    def _preprocess_summary_per_sentence(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            final_sentences = []\n",
    "            current_len = 0\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                for sentence in sentences:\n",
    "                    tokens = sentence.strip().split()\n",
    "                    tokens_len = len(tokens)\n",
    "                    if current_len + tokens_len < self.length_limit:\n",
    "                        sentence = \" \".join(tokens)\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += tokens_len\n",
    "                    else:\n",
    "                        sentence = \" \".join(tokens[: self.length_limit - current_len])\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        sentence = sentence[: self.length_limit - current_len]\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            sentences = final_sentences\n",
    "\n",
    "        final_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence.lower()).strip()\n",
    "            tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence))\n",
    "            sentence = \" \".join(tokens)\n",
    "            final_sentences.append(sentence)\n",
    "\n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd1be8c-f19a-4975-b3b2-d5218819564c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RougeScorer:\n",
    "    def __init__(self):\n",
    "        self.rouge_evaluator = Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            use_tokenizer=True,\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_df, hyp_df):\n",
    "        #ref_df = pd.read_csv(ref_path)\n",
    "        #hyp_df = pd.read_csv(hyp_path)\n",
    "        hyp_df.iloc[:,1] = hyp_df.iloc[:,1].fillna(' ')\n",
    "        ids = ref_df['id']\n",
    "        hyp_df = hyp_df[hyp_df['id'].isin(ids)]\n",
    "        hyp_df.index = ref_df.index\n",
    "\n",
    "        ref_df = ref_df.sort_values(by=[\"id\"])\n",
    "        hyp_df = hyp_df.sort_values(by=[\"id\"])\n",
    "        ref_df[\"id\"] = ref_df[\"id\"].astype(int)\n",
    "        hyp_df[\"id\"] = hyp_df[\"id\"].astype(int)\n",
    "\n",
    "        hyps = [tuple(row) for row in hyp_df.values]\n",
    "        refs = [tuple(row) for row in ref_df.values]\n",
    "\n",
    "        reference_summaries = []\n",
    "        generated_summaries = []\n",
    "\n",
    "        for ref_tp, hyp_tp in zip(refs, hyps):\n",
    "            ref_id, ref = ref_tp\n",
    "            hyp_id, hyp = hyp_tp\n",
    "\n",
    "            assert ref_id == hyp_id\n",
    "\n",
    "            reference_summaries.append(ref)\n",
    "            generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        #self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "        return \"{:.3f},{:.3f},{:.3f}\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3311b471-69ec-41cf-b50f-a1006ce6dfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    rougue = RougeScorer()\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    ans = rougue.compute_rouge(labels, preds)\n",
    "    return {\n",
    "        \"rouge-1\" : ans[0],\n",
    "        \"rouge-2\" : ans[1],\n",
    "        \"rouge-l\" : ans[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ecda25d-f454-4cca-b97e-7acd831c16b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('gogamza/kobart-base-v1').to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5dc9d-e93d-435e-9428-3b1246a69071",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5475b55-7dae-45bf-bc77-9ccdcdec4270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73340\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 2291000\n",
      "  Number of trainable parameters = 123859968\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='2291000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     31/2291000 00:37 < 829:17:31, 0.77 it/s, Epoch 0.01/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.587800</td>\n",
       "      <td>4.571154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.287200</td>\n",
       "      <td>1.688820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.173500</td>\n",
       "      <td>0.601220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models\\checkpoint-10\n",
      "Configuration saved in models\\checkpoint-10\\config.json\n",
      "Model weights saved in models\\checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in models\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in models\\checkpoint-10\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models\\checkpoint-20\n",
      "Configuration saved in models\\checkpoint-20\\config.json\n",
      "Model weights saved in models\\checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in models\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in models\\checkpoint-20\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models\\checkpoint-30\n",
      "Configuration saved in models\\checkpoint-30\\config.json\n",
      "Model weights saved in models\\checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in models\\checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in models\\checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [models\\checkpoint-10] due to args.save_total_limit\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(run_name = f'KoBARTSum',                    # 모델이름\n",
    "                                output_dir= f\"models\",                      # 모델저장경로\n",
    "                                evaluation_strategy=\"steps\",                # 모델의 평가를 언제 진행할지\n",
    "                                eval_steps=10,                              # 500 스텝 마다 모델 평가\n",
    "                                save_steps=10,                              # 500 스텝 마다 모델 저장\n",
    "                                save_total_limit = 2,                       # 저장할 모델의 갯수\n",
    "                                logging_steps=10,                           # 학습로스 로깅\n",
    "                                per_device_train_batch_size=batch,          \n",
    "                                per_device_eval_batch_size=batch,                      \n",
    "                                gradient_accumulation_steps=16,             # 가상배치\n",
    "                                num_train_epochs=epoch,                                \n",
    "                                learning_rate=lr,                                     \n",
    "                                load_best_model_at_end=True,                # 평가기준 스코어가 좋은 모델만 저장할지 여부\n",
    "                                fp16=True,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                predict_with_generate=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         args=args,\n",
    "                         train_dataset=train_dataset,\n",
    "                         eval_dataset=val_dataset,\n",
    "                         compute_metrics=compute_rouge,\n",
    "                         callbacks=[EarlyStoppingCallback(early_stopping_patience=stop)],\n",
    "                         data_collator=collator,) # callback\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861a0a7-b8b3-43c0-bbe9-fc7ca0af502f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
