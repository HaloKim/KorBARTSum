{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f205f077-501c-443b-9621-614f9365c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "import platform\n",
    "import itertools\n",
    "import collections\n",
    "import pkg_resources  # pip install py-rouge\n",
    "from io import open\n",
    "from rouge import Rouge\n",
    "from konlpy.tag import Mecab \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import transformers\n",
    "from transformers import get_scheduler, PreTrainedTokenizerFast, EarlyStoppingCallback, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfed46-d560-4ed1-a404-56476b58d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer arguments\n",
    "lr = 1e-3\n",
    "stop = 3\n",
    "epoch = 100\n",
    "batch = 4\n",
    "seed = 42\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749fa23-4c11-4378-ae4b-7a1f2cf36614",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52af1a5e-1e96-48b7-8ad4-9d4a797debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwsS3Downloader(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aws_access_key_id=None,\n",
    "        aws_secret_access_key=None,\n",
    "    ):\n",
    "        self.resource = boto3.Session(\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        ).resource(\"s3\")\n",
    "        self.client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "        )\n",
    "\n",
    "    def __split_url(self, url: str):\n",
    "        if url.startswith(\"s3://\"):\n",
    "            url = url.replace(\"s3://\", \"\")\n",
    "        bucket, key = url.split(\"/\", maxsplit=1)\n",
    "        return bucket, key\n",
    "\n",
    "    def download(self, url: str, local_dir: str):\n",
    "        bucket, key = self.__split_url(url)\n",
    "        filename = os.path.basename(key)\n",
    "        file_path = os.path.join(local_dir, filename)\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        meta_data = self.client.head_object(Bucket=bucket, Key=key)\n",
    "        total_length = int(meta_data.get(\"ContentLength\", 0))\n",
    "\n",
    "        downloaded = 0\n",
    "\n",
    "        def progress(chunk):\n",
    "            nonlocal downloaded\n",
    "            downloaded += chunk\n",
    "            done = int(50 * downloaded / total_length)\n",
    "            sys.stdout.write(\n",
    "                \"\\r{}[{}{}]\".format(file_path, \"█\" * done, \".\" * (50 - done))\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                self.client.download_fileobj(bucket, key, f, Callback=progress)\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.flush()\n",
    "        except:\n",
    "            raise Exception(f\"downloading file is failed. {url}\")\n",
    "        return file_path\n",
    "\n",
    "def download(url, chksum=None, cachedir=\".cache\"):\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    os.makedirs(cachedir_full, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "    file_path = os.path.join(cachedir_full, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        if hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10] == chksum:\n",
    "            print(f\"using cached model. {file_path}\")\n",
    "            return file_path, True\n",
    "\n",
    "    s3 = AwsS3Downloader()\n",
    "    file_path = s3.download(url, cachedir_full)\n",
    "    if chksum:\n",
    "        assert (\n",
    "            chksum == hashlib.md5(open(file_path, \"rb\").read()).hexdigest()[:10]\n",
    "        ), \"corrupted file!\"\n",
    "    return file_path, False\n",
    "\n",
    "def get_kobart_tokenizer(cachedir=\".cache\"):\n",
    "    \"\"\"Get KoGPT2 Tokenizer file path after downloading\"\"\"\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/tokenizers/kobart_base_tokenizer_cased_cf74400bce.zip\",\n",
    "        \"chksum\": \"cf74400bce\",\n",
    "    }\n",
    "    file_path, is_cached = download(\n",
    "        tokenizer[\"url\"], tokenizer[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    if (\n",
    "        not os.path.exists(os.path.join(cachedir_full, \"emji_tokenizer\"))\n",
    "        or not is_cached\n",
    "    ):\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(\n",
    "                os.path.join(cachedir_full, \"emji_tokenizer\"), ignore_errors=True\n",
    "            )\n",
    "        zipf = ZipFile(os.path.expanduser(file_path))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    tok_path = os.path.join(cachedir_full, \"emji_tokenizer/model.json\")\n",
    "    tokenizer_obj = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tok_path,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "    )\n",
    "    return tokenizer_obj\n",
    "\n",
    "def get_pytorch_kobart_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    pytorch_kobart = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBART/models/kobart_base_cased_ff4bda5738.zip\",\n",
    "        \"chksum\": \"ff4bda5738\",\n",
    "    }\n",
    "    model_zip, is_cached = download(\n",
    "        pytorch_kobart[\"url\"], pytorch_kobart[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.join(os.getcwd(), cachedir)\n",
    "    model_path = os.path.join(cachedir_full, \"kobart_from_pretrained\")\n",
    "    if not os.path.exists(model_path) or not is_cached:\n",
    "        if not is_cached:\n",
    "            shutil.rmtree(model_path, ignore_errors=True)\n",
    "        zipf = ZipFile(os.path.expanduser(model_zip))\n",
    "        zipf.extractall(path=cachedir_full)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d000bf5f-3255-4995-a8b5-0312a863809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(phase):\n",
    "    work_dir = \"C:\\\\Users\\\\hist\\\\Documents\\\\GitHub\\\\KoBART\"\n",
    "    if phase == 'train':\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/1.Training/라벨링데이터/TL1'\n",
    "    else:\n",
    "        tmp = work_dir+'/022.요약문 및 레포트 생성 데이터/01.데이터/2.Validation/라벨링데이터/VL1'\n",
    "    listdir = os.listdir(tmp)\n",
    "    df = pd.DataFrame({}, columns = ['genre', 'text', 'label'])\n",
    "    for i in listdir:\n",
    "        files = os.listdir(f'{tmp}/{i}/2~3sent')\n",
    "        for f in tqdm(files):\n",
    "            with open(f'{tmp}/{i}/2~3sent/{f}', 'r', encoding='utf-8') as json_file:\n",
    "                j = json.loads(json_file.read())\n",
    "                df2 = pd.DataFrame.from_dict([{'genre' : i, \n",
    "                                               'text'  : j['Meta(Refine)']['passage'], \n",
    "                                               'label' : j['Annotation']['summary1']}])\n",
    "                df = pd.concat([df, df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8e76f6-4842-4a72-9aa3-199d485162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train = make_df('train').reset_index(drop=True)\n",
    "# val = make_df('val').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6ee0bd-7fbe-48c4-baa2-9b3437f20c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_parquet('train.parquet')\n",
    "# val.to_parquet('val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c585aecd-26da-4842-a0bb-4c664674508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08.speech</td>\n",
       "      <td>안녕하십니까? 국립산림과학원 임산공학부 화학미생물과 안병준입니다. 산림바이오매스는 ...</td>\n",
       "      <td>신재생에너지 공급 의무화 제도를 시행함에 따라 발전소에서는 새로운 대체에너지 발굴에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06.edit</td>\n",
       "      <td>그가 내가 다음 가야 할 곳을 알려주었다. 나는 서울 외곽의 군부대 같아 보이는 ...</td>\n",
       "      <td>서울 외곽의 군부대처럼 보이는 지역으로 가 철문을 통과해 들어갔고 IQ검사 등을 받았다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       genre                                               text  \\\n",
       "0  08.speech  안녕하십니까? 국립산림과학원 임산공학부 화학미생물과 안병준입니다. 산림바이오매스는 ...   \n",
       "1    06.edit   그가 내가 다음 가야 할 곳을 알려주었다. 나는 서울 외곽의 군부대 같아 보이는 ...   \n",
       "\n",
       "                                               label  \n",
       "0  신재생에너지 공급 의무화 제도를 시행함에 따라 발전소에서는 새로운 대체에너지 발굴에...  \n",
       "1  서울 외곽의 군부대처럼 보이는 지역으로 가 철문을 통과해 들어갔고 IQ검사 등을 받았다.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "val   = pd.read_parquet('val.parquet')\n",
    "val   = val.sample(n=2, replace=False).reset_index(drop=True)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236632ac-68ab-40f1-a345-df7feb5aebd2",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0df920-1331-4896-b591-0f8d1e536fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocss(df):\n",
    "    df.text = df.text.apply(lambda x : re.sub('\\n', ' ',  x))\n",
    "    df.text = df.text.apply(lambda x : re.sub(' +', ' ',  x).strip())\n",
    "    return df\n",
    "\n",
    "train = preprocss(train)\n",
    "val = preprocss(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b593e39-9bad-4a94-9253-a0ecfa3bde08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하십니까? 국립산림과학원 임산공학부 화학미생물과 안병준입니다. 산림바이오매스는 지구의 육상 바이오매스의 90%를 차지하고 있는 미래 에너지원으로서 잠재력이 높은 자원입니다. 정부에서는 2008년에 제1차 국가에너지기본계획을 수립하여 추진하여 왔으며, 금년 1월 14일 2차 국가에너지기본계획을 발표하였습니다. 2차 기본계획에서는 원자력 발전의 비율을 당초 41%에서 29% 수준으로 낮추고 신재생에너지의 비율은 1차 기본계획에서 제시했던 11% 수준에서 추진하는 것으로 공표하였습니다. 또한, 산업통상자원부에서는 2012년부터 매년 50만㎾ 이상을 생산하는 국내 발전사업자를 대상으로 총 발전량의 일부를 신재생에너지로 공급하도록 하는 신재생에너지 공급 의무화 제도를 시행함에 따라, 공급의무 대상자이자 대규모 사용처인 발전소에서는 새로운 대체에너지 발굴에 총력을 기울이고 있는 실정입니다. 그러나 12개의 신재생에너지원중에서 단기간 내에 현장적용이 가능한 에너지원은 바이오 에너지를 비롯한 일부에 국한되어있는 것이 현실입니다. 따라서 목질류의 바이오메스를 포함하는 바이오 에너지의 비율은 매년 지속적으로 증가할것으로 예상되고 있습니다. 산림청 국립산림과학원에서는 대표적인 청정연료인 목재펠릿에 대해 2009년 품질규격을 제정하여 국내 유통제품등에 대해 품질을 관리하고 있으나, 최근 해외로로부터 수입되는 제품증가와 환경문제등을 고려한 새로운 규격품질 기준이 요구되고 있는 실정입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5314e71-d4c6-4854-9a55-f21ba19e5500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신재생에너지 공급 의무화 제도를 시행함에 따라 발전소에서는 새로운 대체에너지 발굴에 총력을 기울이고 있지만 단기간 내에 현장 적용이 가능한 에너지원은 일부에 국한되어 있다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.loc[0, 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d04c8a62-20e5-4e03-a233-d46bd31414b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c8bce-85f3-4df7-86fd-a997b4cf45f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b74843-6691-4e39-acf2-628b7cc0bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "class KoBARTSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "\n",
    "        self.pad_index = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.df.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(instance['text'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tokenizer.encode(instance['label'])\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "        \n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "train_dataset = KoBARTSumDataset(train, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 1024)\n",
    "val_dataset = KoBARTSumDataset(val, PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1'), 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8de69c-1cd7-47e1-959a-52fd76a5212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1499, 776)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().max(), val.text.str.len().max(), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5d583-b424-4621-94dd-18b55d51bebb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecda25d-f454-4cca-b97e-7acd831c16b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('gogamza/kobart-base-v1').to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c5dc9d-e93d-435e-9428-3b1246a69071",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3311b471-69ec-41cf-b50f-a1006ce6dfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    print('Text\\n', val.text[0])\n",
    "    print(f'Summarize\\nGold : {labels[0]}\\nGen : {preds[0]}')\n",
    "    \n",
    "    labels = ['\\n'.join(labels)]\n",
    "    preds = ['\\n'.join(preds)]\n",
    "    score = rouge.get_scores(preds, labels, avg=True)\n",
    "    return {\n",
    "        \"ROUGE-1\" : score['rouge-1'],\n",
    "        \"ROUGE-2\" : score['rouge-2'],\n",
    "        \"ROUGE-L\" : score['rouge-l'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ffaa3fc-c480-4ec4-b3e7-95846b76f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=1, eta_min=0, last_epoch=-1)\n",
    "\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "                                                            num_warmup_steps=0, \n",
    "                                                            num_training_steps=epoch * len(train_dataset) * batch, \n",
    "                                                            last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06715e6-246b-45af-b86c-463d0b260cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 73340\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 114500\n",
      "  Number of trainable parameters = 123859968\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='114500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    15/114500 01:19 < 194:22:59, 0.16 it/s, Epoch 0.01/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.197400</td>\n",
       "      <td>2.470030</td>\n",
       "      <td>{'r': 0.11764705882352941, 'p': 0.2857142857142857, 'f': 0.16666666253472232}</td>\n",
       "      <td>{'r': 0.030303030303030304, 'p': 0.07692307692307693, 'f': 0.04347825681474518}</td>\n",
       "      <td>{'r': 0.11764705882352941, 'p': 0.2857142857142857, 'f': 0.16666666253472232}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 4\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " 안녕하십니까? 국립산림과학원 임산공학부 화학미생물과 안병준입니다. 산림바이오매스는 지구의 육상 바이오매스의 90%를 차지하고 있는 미래 에너지원으로서 잠재력이 높은 자원입니다. 정부에서는 2008년에 제1차 국가에너지기본계획을 수립하여 추진하여 왔으며, 금년 1월 14일 2차 국가에너지기본계획을 발표하였습니다. 2차 기본계획에서는 원자력 발전의 비율을 당초 41%에서 29% 수준으로 낮추고 신재생에너지의 비율은 1차 기본계획에서 제시했던 11% 수준에서 추진하는 것으로 공표하였습니다. 또한, 산업통상자원부에서는 2012년부터 매년 50만㎾ 이상을 생산하는 국내 발전사업자를 대상으로 총 발전량의 일부를 신재생에너지로 공급하도록 하는 신재생에너지 공급 의무화 제도를 시행함에 따라, 공급의무 대상자이자 대규모 사용처인 발전소에서는 새로운 대체에너지 발굴에 총력을 기울이고 있는 실정입니다. 그러나 12개의 신재생에너지원중에서 단기간 내에 현장적용이 가능한 에너지원은 바이오 에너지를 비롯한 일부에 국한되어있는 것이 현실입니다. 따라서 목질류의 바이오메스를 포함하는 바이오 에너지의 비율은 매년 지속적으로 증가할것으로 예상되고 있습니다. 산림청 국립산림과학원에서는 대표적인 청정연료인 목재펠릿에 대해 2009년 품질규격을 제정하여 국내 유통제품등에 대해 품질을 관리하고 있으나, 최근 해외로로부터 수입되는 제품증가와 환경문제등을 고려한 새로운 규격품질 기준이 요구되고 있는 실정입니다.\n",
      "Summarize\n",
      "Gold : 신재생에너지 공급 의무화 제도를 시행함에 따라 발전소에서는 새로운 대체에너지 발굴에 총력을 기울이고 있지만 단기간 내에 현장 적용이 가능한 에너지원은 일부에 국한되어 있다.\n",
      "Gen : 산림청림과학원 임산공학부 화학미생물과 안병준입니다. 산림\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models\\checkpoint-10\n",
      "Configuration saved in models\\checkpoint-10\\config.json\n",
      "Configuration saved in models\\checkpoint-10\\generation_config.json\n",
      "Model weights saved in models\\checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in models\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in models\\checkpoint-10\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(run_name = f'KoBARTSum',\n",
    "                                output_dir= f\"models\",\n",
    "                                evaluation_strategy=\"steps\",\n",
    "                                eval_steps=10,\n",
    "                                save_steps=10,\n",
    "                                logging_steps=10,\n",
    "                                save_total_limit = 2,\n",
    "                                \n",
    "                                per_device_train_batch_size=batch,\n",
    "                                per_device_eval_batch_size=batch,\n",
    "                                gradient_accumulation_steps=16,\n",
    "                                num_train_epochs=epoch,\n",
    "                                \n",
    "                                load_best_model_at_end=True,\n",
    "                                fp16=True,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                predict_with_generate=True,)\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         args=args,\n",
    "\n",
    "                         train_dataset=train_dataset,\n",
    "                         eval_dataset=val_dataset,\n",
    "\n",
    "                         compute_metrics=compute_metrics,\n",
    "                         optimizers=(optimizer, lr_scheduler),\n",
    "                         callbacks=[EarlyStoppingCallback(early_stopping_patience=stop)],\n",
    "                         data_collator=collator,)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bcfb0-64aa-4442-9a69-bfa7dee75541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91609e7-cddf-4b59-93df-8694f525c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
